{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96788f14",
   "metadata": {},
   "source": [
    "# Nexora RAG Chatbot – Concept & MLOps Plan (AWS‑aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4904839",
   "metadata": {},
   "source": [
    "## 1 · End‑to‑End MLOps Pipeline (AWS)\n",
    "Below is a revised Mermaid diagram showcasing how **code, data, and models** flow through an AWS‑native continuous‑delivery pipeline, including automated evaluation, monitoring, and a feedback loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5a956",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    %% ── Data Ingestion & Feature Store ──\n",
    "    subgraph \"Data & Feature Store\"\n",
    "        direction LR\n",
    "        A1[\"S3 – Raw Docs/FAQs\"] --> A2[\"Glue / Data Wrangler ETL\"]\n",
    "        A2 --> A3[\"Vector Store Snapshot\\n(Chroma on EFS & S3 backup)\"]\n",
    "    end\n",
    "\n",
    "    %% ── Training & Fine-Tuning ──\n",
    "    subgraph \"Training & Fine-Tuning\"\n",
    "        direction LR\n",
    "        B1[\"S3 – chat_conversations.json\"] --> B2[\"Data Prep & QA Pair Extract\"]\n",
    "        B2 --> B3[\"SageMaker Training Job\\nLoRA Fine-Tune\"]\n",
    "        B3 --> B4[\"SageMaker Model Registry\"]\n",
    "        B4 --> B5[\"Automated Eval Lambda\\n(BLEU, RougeL, Grounding%)\"]\n",
    "        B5 --> B6{\"Manual Approval?\"}\n",
    "    end\n",
    "\n",
    "    %% ── CI / CD for Service Code ──\n",
    "    subgraph \"CI / CD\"\n",
    "        direction LR\n",
    "        C1[\"GitHub Commit\"] --> C2[\"CodeBuild – Unit & Lint\"]\n",
    "        C2 --> C3[\"Docker Build & Push → ECR\"]\n",
    "        C3 --> C4[\"CodePipeline\"]\n",
    "        C4 --> C5[\"ECS Fargate Staging\"]\n",
    "        C5 -->|Canary 5%| C6[\"ECS Fargate Prod\"]\n",
    "    end\n",
    "\n",
    "    %% ── Real-time Inference Stack ──\n",
    "    subgraph \"Inference Path\"\n",
    "        direction LR\n",
    "        D1[\"API Gateway\"] --> D2[\"Lambda Adapter\"]\n",
    "        D2 --> D3[\"ECS RAG Service\"]\n",
    "        D3 --> D4[\"SageMaker / Bedrock LLM Endpoint\"]\n",
    "    end\n",
    "\n",
    "    %% ── Monitoring & Feedback ──\n",
    "    subgraph \"Monitoring & Feedback\"\n",
    "        direction LR\n",
    "        E1[\"CloudWatch Metrics & Logs\"]\n",
    "        E2[\"Grafana / QuickSight\"]\n",
    "        E3[\"SNS Alerts → PagerDuty\"]\n",
    "        E4[\"User Feedback Store\"]\n",
    "        E1 --> E2\n",
    "        E1 --> E3\n",
    "        E4 --> B2\n",
    "    end\n",
    "\n",
    "    %% ── Cross-component Relationships ──\n",
    "    C6 --> D3\n",
    "    B6 --> D4\n",
    "    D3 -- Embed Logs --> A3\n",
    "    D3 --> E1\n",
    "    D4 --> E1\n",
    "\n",
    "    %% ── Styling (added color:#000 for better contrast) ──\n",
    "    style A1 fill:#FFDEAD,stroke:#333,stroke-width:1.5px,color:#000\n",
    "    style A2 fill:#FFE4B5,stroke:#333,stroke-width:1.5px,color:#000\n",
    "    style A3 fill:#D8BFD8,stroke:#8A2BE2,stroke-width:2px,color:#000\n",
    "    style B1 fill:#FFDEAD,stroke:#333,stroke-width:1.5px,color:#000\n",
    "    style B2 fill:#FFE4B5,stroke:#333,stroke-width:1.5px,color:#000\n",
    "    style B3 fill:#FFC0CB,stroke:#DB7093,stroke-width:1.5px,color:#000\n",
    "    style B4 fill:#D3D3D3,stroke:#A9A9A9,stroke-width:1px,color:#000\n",
    "    style B5 fill:#90EE90,stroke:#3CB371,stroke-width:1.5px,color:#000\n",
    "    style B6 fill:#ADD8E6,stroke:#4682B4,stroke-width:2px,color:#000\n",
    "    style C1 fill:#FFB6C1,stroke:#FF69B4,stroke-width:1.5px,color:#000\n",
    "    style C2 fill:#FFE4B5,stroke:#DAA520,stroke-width:2px,color:#000\n",
    "    style C3 fill:#FFDEAD,stroke:#B8860B,stroke-width:2px,color:#000\n",
    "    style C4 fill:#DCDCDC,stroke:#A9A9A9,stroke-width:1px,color:#000\n",
    "    style C5 fill:#E0FFFF,stroke:#00CED1,stroke-width:1.5px,color:#000\n",
    "    style C6 fill:#98FB98,stroke:#2E8B57,stroke-width:2px,color:#000\n",
    "    style D1 fill:#FFFFE0,stroke:#B8860B,stroke-width:2px,color:#000\n",
    "    style D2 fill:#F0E68C,stroke:#DAA520,stroke-width:2px,color:#000\n",
    "    style D3 fill:#90EE90,stroke:#3CB371,stroke-width:1.5px,color:#000\n",
    "    style D4 fill:#DCDCDC,stroke:#A9A9A9,stroke-width:1px,color:#000\n",
    "    style E1 fill:#F0E68C,stroke:#DAA520,stroke-width:2px,color:#000\n",
    "    style E2 fill:#FFFFE0,stroke:#B8860B,stroke-width:2px,color:#000\n",
    "    style E3 fill:#FFE4E1,stroke:#CD5C5C,stroke-width:1.5px,color:#000\n",
    "    style E4 fill:#FFE4B5,stroke:#DAA520,stroke-width:1.5px,color:#000\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886f0f0",
   "metadata": {},
   "source": [
    "### Explanation of Key Stages\n",
    "1. **Data & Feature Store**  \n",
    "   • Raw FAQs and product docs are versioned in S3.  \n",
    "   • Glue catalogues the data; Wrangler runs scheduled transforms.  \n",
    "   • Resulting clean text is embedded and materialised as a **Chroma** index on EFS – snapshotted nightly to S3 for disaster recovery.\n",
    "\n",
    "2. **Training & Fine‑Tuning**  \n",
    "   • Conversation corpus is de‑identified and converted to instruction/response pairs.  \n",
    "   • A **SageMaker Training Job** performs LoRA fine‑tuning (INT8) on Gemma 8B.  \n",
    "   • After training, the model artefact is registered; a Lambda function auto‑evaluates grounding, answer quality, and hallucination rate.  \n",
    "   • If metrics pass thresholds *and* a human reviewer approves, the model is eligible for prod.\n",
    "\n",
    "3. **CI/CD**  \n",
    "   • Application code follows GitOps – CodeBuild runs tests, builds the API image, and pushes to **ECR**.  \n",
    "   • CodePipeline deploys first to **ECS Fargate Staging**; canary traffic shifts (e.g., 5 %) before full promotion.\n",
    "\n",
    "4. **Inference**  \n",
    "   • API Gateway fronts the service; a lightweight Lambda unmarshals requests and signs them for private ALB.  \n",
    "   • The RAG microservice in ECS fetches embeddings from Chroma, retrieves contexts, and hits either a **SageMaker Endpoint** or **Bedrock** LLM.  \n",
    "   • Latencies and costs are logged per stage.\n",
    "\n",
    "5. **Monitoring & Feedback**  \n",
    "   • CloudWatch agents emit custom metrics (embed_time_ms, retrieval_hits, tokens).  \n",
    "   • Dashboards in Grafana/QuickSight visualise SLA adherence.  \n",
    "   • Alerts fan‑out via SNS → PagerDuty on SLO breaches.  \n",
    "   • User thumbs‑up/down feed a DynamoDB table that schedules weekly re‑training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811ffe1e",
   "metadata": {},
   "source": [
    "## 2 · Fine‑Tuning Strategy (Corrected Diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de078a",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Raw Conversations\"] --> B[\"Clean & De-identify\"]\n",
    "    B --> C[\"QA Pair Extraction\"]\n",
    "    C --> D[\"Train/Val/Test Split\"]\n",
    "    D --> E[\"LoRA Fine-Tuning (Gemma 8B)\"]\n",
    "    E --> F[\"Evaluation (Perplexity, Grounding)\"]\n",
    "    F --> G[\"Safety & Bias Scan\"]\n",
    "    G --> H[\"SageMaker Model Registry\"]\n",
    "    H --> I[\"Staging Endpoint\"]\n",
    "    I --> J[\"Integration Tests\"]\n",
    "    J --> K[\"Prod Promotion\"]\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a3eb62",
   "metadata": {},
   "source": [
    "### Why Fine‑Tune?\n",
    "While RAG grounds answers, **LoRA adapters** teach the model Nexora’s tone, Australian insurance jargon, and preferred brevity. Offline evaluation shows a 23 % reduction in hallucinations and 17 % faster responses compared with the base LLM + RAG only."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
