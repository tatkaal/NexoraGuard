{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0a6a7e",
   "metadata": {},
   "source": [
    "# Nexora RAG Customer Service Chatbot – Demo & Implementation Guide\n",
    "**Author:** Sujan Adhikari\n",
    "\n",
    "This interactive notebook walks through building a Retrieval‑Augmented Generation (RAG) customer‑service chatbot for **Nexora Pty Ltd** using **LangChain**, **Ollama Embeddings + Qwen3:1.7b**, and **ChromaDB**.  It covers:\n",
    "1. Data loading & preprocessing  \n",
    "2. Vector‑store creation  \n",
    "3. Simple intent recognition & entity extraction  \n",
    "4. RAG pipeline assembly  \n",
    "5. Lightweight Gradio web‑chat demo  \n",
    "6. (Optional) pointers for deployment & MLOps alignment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d98e832",
   "metadata": {},
   "source": [
    "## 0  Environment & Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e821abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain-chroma langchain-ollama chromadb ollama gradio spacy pandas \n",
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82259e29",
   "metadata": {},
   "source": [
    "## 1  Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deaf0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, pandas as pd, csv\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import spacy, gradio as gr\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf3a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# configure ollama model & embeddings\n",
    "MODEL = 'qwen3:1.7b'\n",
    "EMBEDDINGS_MODEL = \"mxbai-embed-large\"\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb07f0dd",
   "metadata": {},
   "source": [
    "## 2  Fix the faqs.csv dataset since there are questions separated by comma and not enclosed in quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b606c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"data/faqs.csv\")\n",
    "output_path = Path(\"data/faqs_cleaned.csv\")\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    header = next(reader)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for row in reader:\n",
    "        if len(row) > 3:\n",
    "            # Fix rows with a comma in the question (merge columns until we have 3)\n",
    "            question_parts = row[:-2]  # Everything except last 2 fields\n",
    "            question = \",\".join(question_parts).strip()\n",
    "            answer = row[-2].strip()\n",
    "            category = row[-1].strip()\n",
    "            writer.writerow([question, answer, category])\n",
    "        else:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063982cb",
   "metadata": {},
   "source": [
    "## 2  Data Ingestion & Preprocessing: The Knowledge Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e12cd75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 136\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path('data')  # adjust if different\n",
    "products_path = DATA_DIR / 'products_occupation.json'\n",
    "faqs_path = DATA_DIR / 'faqs_cleaned.csv'\n",
    "\n",
    "# Load products\n",
    "with open(products_path, 'r', encoding='utf-8') as f:\n",
    "    products_data = json.load(f)['products']\n",
    "\n",
    "# Flatten products into plain‑text docs\n",
    "product_docs = []\n",
    "for p in products_data:\n",
    "    text = f\"\"\"\n",
    "    Product Name: {p['name']}\n",
    "    Description: {p['description']}\n",
    "    Target Industries: {', '.join(p['target_industries'])}\n",
    "    Coverage Options: {', '.join(p['coverage_options'])}\n",
    "    Premium Range: {p['premium_range']['min']}-{p['premium_range']['max']} {p['premium_range']['currency']}\n",
    "    Excess Range: {p['excess_range']['min']}-{p['excess_range']['max']} {p['excess_range']['currency']}\n",
    "    Key Features: {', '.join(p['key_features'])}\n",
    "    Exclusions: {', '.join(p['exclusions'])}\n",
    "    Unique Selling Points: {', '.join(p['unique_selling_points'])}\n",
    "    Required Documents: {', '.join(p['required_documents'])}\n",
    "    \"\"\".strip()\n",
    "    product_docs.append(Document(page_content=text, metadata={'type': 'product', 'name': p['name']}))\n",
    "\n",
    "\n",
    "# Load FAQs\n",
    "faqs_df = pd.read_csv(faqs_path, quotechar='\"')\n",
    "\n",
    "# ✅ Validate the structure of the CSV\n",
    "assert set(faqs_df.columns) == {'question', 'answer', 'category'}, \"Unexpected CSV format\"\n",
    "\n",
    "faq_docs = [\n",
    "    Document(\n",
    "        page_content=f\"Question: {row.question}\\nAnswer: {row.answer}\",\n",
    "        metadata={'type': 'faq', 'category': row.category}\n",
    "    )\n",
    "    for _, row in faqs_df.iterrows()\n",
    "]\n",
    "\n",
    "# Combine all documents\n",
    "documents = product_docs + faq_docs\n",
    "print(f'Total documents: {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af9e0e",
   "metadata": {},
   "source": [
    "## 3  Create / Reload Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ed98ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing vector store.\n"
     ]
    }
   ],
   "source": [
    "# Set up vector store\n",
    "VECTOR_DIR = 'nexora_chroma'\n",
    "\n",
    "if Path(VECTOR_DIR).exists():\n",
    "    vector_store = Chroma(persist_directory=VECTOR_DIR, embedding_function=embeddings)\n",
    "    print('Loaded existing vector store.')\n",
    "else:\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=VECTOR_DIR\n",
    "    )\n",
    "    print('Built new vector store →', VECTOR_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376b544",
   "metadata": {},
   "source": [
    "## 4  Lightweight Intent Recognition & Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eac9086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claims\n",
      "{'products': ['professional indemnity'], 'industries': ['Professional Indemnity']}\n"
     ]
    }
   ],
   "source": [
    "INTENT_PATTERNS = {\n",
    "    'claims': r'\\b(claim|lodg(e|ing)|damage)\\b',\n",
    "    'coverage': r'\\b(cover(ed|age)|policy limit|exclusion|excess)\\b',\n",
    "    'products': r'\\bwhat is [A-Za-z ]+ insurance|types? of insurance\\b',\n",
    "    'pricing': r'\\b(cost|price|premium|fee)\\b',\n",
    "    'account': r'\\b(login|account|certificate of currency|policy documents|amend)\\b',\n",
    "}\n",
    "\n",
    "PRODUCT_NAMES = [p['name'].lower() for p in products_data]\n",
    "\n",
    "def classify_intent(query:str):\n",
    "    for intent, pattern in INTENT_PATTERNS.items():\n",
    "        if re.search(pattern, query, re.IGNORECASE):\n",
    "            return intent\n",
    "    return 'general'\n",
    "\n",
    "def extract_entities(query:str):\n",
    "    doc = nlp(query)\n",
    "    products = [p for p in PRODUCT_NAMES if p in query.lower()]\n",
    "    industries = [ent.text for ent in doc.ents if ent.label_=='ORG' or ent.label_=='NORP']\n",
    "    return {'products': products, 'industries': industries}\n",
    "\n",
    "print(classify_intent('How do I lodge a claim?'))\n",
    "print(extract_entities('Do you cover Architecture firms for Professional Indemnity?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92bc02",
   "metadata": {},
   "source": [
    "## 5  Build Retrieval‑Augmented QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ae3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=MODEL, temperature=0, num_ctx=10000, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "prompt_template = (\n",
    "    \"You are NexoraBot, a helpful and knowledgeable customer-service assistant for an Australian SME insurance broker. \"\n",
    "    \"Use ONLY the provided context to answer. If unsure, say you don't know but can escalate to a human agent. \"\n",
    "    \"Answer in a friendly, concise manner and, where relevant, suggest next steps inside Nexora's platform.\\n\\n\"\n",
    "    \"### Context\\n{context}\\n\\n### Question\\n{question}\\n\\n### Answer\\n\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=vector_store.as_retriever(search_kwargs={'k':4}),\n",
    "    chain_type_kwargs={'prompt': prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5629161",
   "metadata": {},
   "source": [
    "### Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99b2f4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerad\\AppData\\Local\\Temp\\ipykernel_17588\\3082393474.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(rag_chain.run(question))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking about what Professional Indemnity cover entails. Let me check the provided context first.\n",
      "\n",
      "Looking at the context, the answers given are about Public Liability, Personal Accident Insurance, and whether a policy needs to start immediately. There's no direct mention of Professional Indemnity. The user's question is specifically about Professional Indemnity, which is a type of insurance coverage.\n",
      "\n",
      "Since the context doesn't include information about Professional Indemnity, I need to determine if I can answer based on the given data. The answer should be concise and friendly. The user might be expecting a standard explanation, but since the context doesn't cover it, I should mention that the information isn't available in the provided context and suggest escalating to a human agent.\n",
      "\n",
      "I should also make sure to follow the instructions: if unsure, say I don't know and escalate. So, the answer would state that the context doesn't cover Professional Indemnity and recommend contacting a human agent for more details. Additionally, I should suggest next steps inside Nexora's platform, like contacting support or a human agent.\n",
      "</think>\n",
      "\n",
      "NexoraBot: Professional Indemnity insurance covers claims of negligence, errors, or omissions made by you or your organization in providing services or products. It protects against financial loss from such incidents.  \n",
      "\n",
      "**Next Step:** Contact Nexora’s support team for detailed guidance on Professional Indemnity coverage.\n"
     ]
    }
   ],
   "source": [
    "question = 'What does Professional Indemnity cover?'\n",
    "print(rag_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78eda906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What does Professional Indemnity cover?', 'result': \"<think>\\nOkay, the user is asking about what Professional Indemnity cover entails. Let me check the provided context first.\\n\\nLooking at the context, the answers given are about Public Liability, Personal Accident Insurance, and whether a policy needs to start immediately. There's no direct mention of Professional Indemnity. The user's question is specifically about Professional Indemnity, which is a type of insurance coverage.\\n\\nSince the context doesn't include information about Professional Indemnity, I need to determine if I can answer based on the given data. The answer should be concise and friendly. The user might be expecting a standard explanation, but since the context doesn't cover it, I should mention that the information isn't available in the provided context and suggest escalating to a human agent.\\n\\nI should also make sure to follow the instructions: if unsure, say I don't know and escalate. So, the answer would state that the context doesn't cover Professional Indemnity and recommend contacting a human agent for more details. Additionally, I should suggest next steps inside Nexora's platform, like contacting support or a human agent.\\n</think>\\n\\nNexoraBot: Professional Indemnity insurance covers claims of negligence, errors, or omissions made by you or your organization in providing services or products. It protects against financial loss from such incidents.  \\n\\n**Next Step:** Contact Nexora’s support team for detailed guidance on Professional Indemnity coverage.\"}\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke({'query': question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f4540",
   "metadata": {},
   "source": [
    "## 6  Gradio Web Chat Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b4697ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zerad\\AppData\\Local\\Temp\\ipykernel_17588\\3318006750.py:13: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label='Chatbot')  # Removed type='messages' for compatibility\n"
     ]
    }
   ],
   "source": [
    "def chat_function(message, history):\n",
    "    intent = classify_intent(message)\n",
    "    entities = extract_entities(message)\n",
    "\n",
    "    result = rag_chain.invoke(message)\n",
    "    answer = result[\"result\"]\n",
    "\n",
    "    meta_note = f\"\\n\\n_Bot note: intent={intent}, entities={entities}_\"\n",
    "    return answer + meta_note\n",
    "\n",
    "with gr.Blocks(title='Nexora Chatbot Demo') as demo:\n",
    "    gr.Markdown('# Nexora Insurance Chatbot')\n",
    "    chatbot = gr.Chatbot(type='messages')\n",
    "    msg = gr.Textbox(placeholder='Ask a question about your policy…', label='Your message')\n",
    "    send = gr.Button('Send')\n",
    "    state = gr.State([])  # List[Tuple[str, str]]\n",
    "\n",
    "    def respond(user_message, chat_history):\n",
    "        bot_reply = chat_function(user_message, chat_history)\n",
    "        chat_history.append((user_message, bot_reply))\n",
    "        return '', chat_history, chat_history\n",
    "\n",
    "    send.click(respond, [msg, state], [msg, chatbot, state])\n",
    "\n",
    "# To launch the interface inside notebook use demo.launch(debug=True)\n",
    "# Or from CLI: `python -m gradio app.py` if saved separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74c9214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655ad65",
   "metadata": {},
   "source": [
    "## 7  Deployment & Next Steps (Conceptual)\n",
    "* **FastAPI + Uvicorn** wrapper to expose `chat_function` as `/chat` endpoint.  \n",
    "* **Docker** containerization with Ollama serving model weights locally.  \n",
    "* **CI/CD** via GitHub Actions → build, push image to GHCR, deploy to Azure Container Apps.  \n",
    "* **Monitoring**: Prometheus + Grafana for latency, Chroma vector‑store health; OpenTelemetry traces for LLM calls.  \n",
    "* **Fine‑Tuning**: see `finetune_plan.md` (not included) for leveraging annotated conversations to supervise‑fine‑tune Llama‑3 using QLoRA.\n",
    "\n",
    "---\n",
    "**Enjoy experimenting with your Nexora chatbot!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
